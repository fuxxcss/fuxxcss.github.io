---
title: "Gzfuzz: 基于强化学习的Gazebo模糊测试工具"
excerpt: 'ISSTA 2025, Reinforcement Learning-Based Fuzz Testing for the Gazebo Robotic Simulator'

collection: learning
category: paper
permalink: /learning/2025-issta-gzfuzz
tags: 
  - issta
  - fuzz
  - gazebo

layout: single
read_time: true
author_profile: false
comments: true
share: true
related: true
---

![](../images/learning/2025_issta_gzfuzz/cover.png)

## Gazebo 

### 一、应用场景

Gazebo是使用最广泛的机器人模拟器，为在高保真 3D 环境中开发、测试和验证机器人系统提供了一个强大而灵活的平台，是 ROS2 的默认模拟器。其应用范围广泛，例如 NASA 太空任务，DARPA 地下探索和自动驾驶。

与其他复杂的软件系统一样，Gazebo 容易出现可能产生严重后果的错误，通过模拟不准确、测试失败和崩溃来影响机器人开发。例如，在数字孪生系统中，模拟器崩溃可能破坏虚拟和物理模型之间的同步，导致意外停机或操作问题。

### 二、模糊测试的挑战

直接将现有的这些模糊测试方法移植到 Gazebo 可能不会产生令人满意的结果。原因在于 Gazebo 使用复杂的输入格式和基于插件的复杂客户端-服务器架构来实现实时物理计算，如下图所示。

![](../images/learning/2025_issta_gzfuzz/gazebo.png)

这些特性使得标准模糊测试工具难以有效探索 Gazebo 中的潜在错误范围。特别是，我们确定了两个主要挑战，描述如下：
- 严格输入挑战（语法语义限制）：Gazebo 服务器的输入有两种形式，模拟描述格式(SDF) 文件和外部命令。SDF 文件使用基于 XML 的语法来定义用于机器人模拟、可视化和控制的对象和环境，而外部命令利用 Protobuf 规范与服务器通信。输入必须遵循严格的语法和语义规则才能与模拟交互。例如，生成机器人的命令必须包含有效的机器人模型、定位数据以及其他参数（如物理属性）。
- 状态空间挑战（代码路径限制）：Gazebo 的模拟过程可能具有高度复杂和庞大的状态空间，涉及使用插件的不同模型、关节、传感器和物理交互，例如，用于控制机器人、模拟摄像头和 LiDAR 传感器以及与环境交互的插件。插件和其他组件共同工作以模拟现实世界的交互，但它们的复杂性产生了巨大的状态空间。

### 三、应对挑战

为了应对这些挑战，本文提出了第一个 GaZebo fuzzing 方法 GzFuzz：
- 利用语法感知的可行命令生成机制，确保模糊测试的输入，特别是 Gazebo 命令，在模拟环境中语法上是有效的。
- 基于学习的命令生成器选择机制，该机制基于多维反馈来指导模糊测试过程。通过使用强化学习，GzFuzz 学习哪些类型的命令或命令序列更有可能探索 Gazebo 模拟环境中未经测试或探索不足的状态。

## GzFuzz

GzFuzz开源在[github](https://github.com/liyitao-code/GzFuzz)。

### 一、框架

GzFuzz 的工作流程如下图所示。

![](../images/learning/2025_issta_gzfuzz/gzfuzz.png)

给定一个 SDF 文件，模糊测试过程如下工作：
- 1.提取 SDF 文件的特征。
- 2-4.使用强化学习框架生成一系列命令生成器选择。
- 5-6.基于这些选择，借助挖掘的模型、插件和 Protobuf 定义，将 Gazebo 命令构建为客户端命令行字符串。
- 7.生成一个 Gazebo 服务器进程，并执行生成的命令与服务器通信。执行后，我们能够收集测试结果。
- 8.分析测试结果，并相应地更新强化学习模块。

关于测试预言，我们主要关注与崩溃相关的错误，因为它们的严重性、清晰性和可重现性。

### 二、实现

**1.预处理**

为了实现模糊测试过程，初始步骤是收集一组种子 SDF 文件。在预处理阶段，我们首先遍历 Gazebo 的源代码仓库，并提取了总共 309 个 SDF 文件用于分析和模糊测试目的。然后，我们使用 lxml (https://pypi.org/project/lxml)库将所有 SDF 文件解析为文档对象模型 (DOM) 树。更具体地说，Gazebo 区分两种类型的插件：附加到 world 节点的插件和附加到 model 节点的插件。我们从 SDF 文件中提取这两种类型的插件，总共收集了 117 个插件。

在下图中，我们展示了一个 SDF 文件示例、其解析后的 DOM 树以及在 Gazebo 中渲染的场景。

![](../images/learning/2025_issta_gzfuzz/sdf.png)

在 SDF 文件中，\<world\> 节点封装了整个环境。在 world 内部，\<model\> 节点代表单个对象或机器人，它们由 \<link\> 节点组成，定义了物理组件，如底盘或车轮。这些链接通过 \<joint\> 节点相互连接，允许机械关节连接。此外，\<plugin\> 节点提供了注入自定义功能的能力，例如用于控制车辆转向的 gz-sim-ackermann-steering-system。

预处理步骤确保我们在模糊测试过程中拥有多样化的模型和插件集，用于生成有意义且有效的命令。在本研究中，我们考虑使用外部命令对 Gazebo 进行模糊测试，而**没有直接变异SDF文件**，出于以下考虑：
- 首先，通过外部命令进行模糊测试允许与 Gazebo 服务器进行更动态的交互，在自动化范式中模拟真实世界的使用场景。与在模拟启动时加载的静态 SDF 文件不同，服务和主题提供了一种连续、实时的操纵模拟的方法，从而带来更即时和多样化的反馈。
- 其次，也可以通过调用 /world/.../create 服务来对 SDF 文件进行模糊测试，该服务允许在模拟期间传入不同的 SDF 内容。

**2.语法感知的可行命令生成**

必须确保生成的命令在模拟环境中语法上是有效的。为了实现这一点，我们根据 Gazebo 命令的特性和预期的测试目标将其分为三个不同的组，即随机生成的命令、半现实命令和破坏性命令。每个类别在发现 Gazebo 中不同类型的问题方面都有特定目的：
- 随机生成的命令：这些命令自动生成以符合 Gazebo 在 Protobuf 格式下的服务和主题要求。目标是探索具有语法有效但不可预测消息的广泛输入空间，帮助发现边缘情况和潜在的系统崩溃。以随机服务为例，命令生成如下：
1. 首先，使用命令 gz service -l 获取候选服务。
2. 然后，随机选择一个服务名称 SN，并通过 gz service -s SN -i 检索所需的消息类型。
3. 有了消息类型，就可以使用第三方库 randomproto(https://pypi.org/project/randomproto)生成消息。
- 半现实命令：此类别模拟典型的用户操作，例如在模拟中添加或删除模型、附加插件或移动对象。例如，要生成一个用于插入带有插件的模型的命令，我们首先在预处理期间准备候选模型。例如，我们可以调用 /world/.../create 服务并传入 SDF 模型，以处理带有插件的模型的实际插入。类似地，要生成插入不带插件的原始模型的命令，我们利用 libsdfformat (https://github.com/gazebosim/sdformat)，并实现链接、碰撞和关节的构建。
- 破坏性命令：在半现实命令的基础上，破坏性命令引入了扰动，例如修改插件参数或发出无效操作（例如，用可能无效的值随机变异插件 DOM 树的叶节点）。

在下图中，我们总结了本研究中设计的命令生成器。所提出的生成器经过精心设计：生成器 1-2 可以通过随机生成的命令覆盖所有可用的服务和主题。生成器 3-7 模拟现实用例，以测试 Gazebo 在正常条件下的表现。生成器 8-10 旨在对 Gazebo 的错误处理能力和处理极端或不正确输入的能力进行压力测试。值得注意的是，生成器 6-9 需要额外的参数来生成相应的命令。例如，生成器 6 用于插入带有插件的模型，其中模型是从 Gazebo 源代码仓库中挖掘的。

![](../images/learning/2025_issta_gzfuzz/generators.png)

**3.基于学习的命令生成器选择**

如前一节所述，命令生成器可能需要参数，这本质上是在选择生成器之后需要进一步做出的决策。由于模糊测试是一个典型的迭代过程，在迭代 tt 时，命令生成器序列表示为 ct={⟨ct1,pt1⟩,⟨ct2,pt2⟩,…,⟨ctne,ptne⟩}ct​={⟨ct1​,pt1​⟩,⟨ct2​,pt2​⟩,…,⟨ctne​​,ptne​​⟩}，其中 cticti​ 表示第 ii 个命令生成器的生成器索引，ptipti​ 表示第 ii 个生成器的参数索引，nene​ 是序列的长度。如果生成器 cticti​ 不需要参数，则 pti=Nonepti​=None。此外，每个序列 cc 伴随一个向量 VV，其中每个元素 vivi​ 表示第 ii 个命令生成器在 cc 中的出现次数。

由于需要选择命令生成器及其参数，我们采用分层强化学习 (HRL) 来对 Gazebo 进行模糊测试，因为它能够有效管理复杂的决策过程并探索大型状态空间。

在本研究中，命令生成器选择包括三个组成部分，即一个行动者模块、一个评判者模块和一组子行动者模块。更具体地说，我们采用两级决策策略：一个行动者用于选择命令生成器，一组子行动者用于选择参数，所有这些都由一个共享的评判者模块指导。每个模块都使用全连接神经网络实现，输入层接收特征向量，后面是一个包含 N 个神经元的隐藏层。行动者/子行动者输出生成器或参数上的概率分布，而评判者预测当前输入的标量奖励值。

对于行动者、子行动者和评判者模块，输入是从 SDF 文件中提取的特征向量，主要通过捕获 Gazebo 模拟场景中的各种实体统计信息来表征输入状态。这些数值特征包括，例如，不带插件的模型数量、带插件的模型数量、模型内的插件数量以及 world 节点下的插件数量。

对于每个模块，给定一个输入向量 s，具有 N 个神经元的隐藏层使用权重矩阵 W1​ 和偏置向量 b1​。隐藏层输出计算如下：

ℎ = ReLU (𝑊1 · 𝑠 + 𝑏1).

从隐藏层到输出，行动者生成一个长度为 𝑚 的一维向量，其中 𝑚 表示可用的生成器选择的数量。该向量作为基于输入特征 𝑠 选择每个生成器的概率分布。权重矩阵𝑊2的维数是 𝑚 × 𝑁 操作，而偏置向量𝑏2的维数是𝑚。输出计算如下：

y𝑎𝑐𝑡𝑜𝑟 = Softmax (𝑊2 · ℎ + 𝑏2).

子行动者的工作方式与行动者类似，除了它们用于生成器的参数之外。对于评判者模块，输出表示当前迭代的估计奖励，使用权重矩阵𝑊3和偏差标量𝑏3。输出为：

y𝑐𝑟𝑖𝑡𝑖𝑐 = 𝑊3 · ℎ + 𝑏3.

在执行命令序列之后，行动者和评判者模块将被更新。更详细地说，总奖励是三个组成部分的总和：
- Crash奖励是为了鼓励找到不同的Crash序列，其中𝜔> 0是一个参数，nt 表示在迭代 t 之前触发相同崩溃类型的次数：
$$
𝑅_{𝑐𝑟𝑎𝑠ℎ}^t = 𝜔 × 𝑛𝑡
$$
- 覆盖率奖励鼓励增加代码覆盖率，其中 𝜆 > 0 是奖励覆盖率改进的参数：
$$
R_{cov}^t = 
\begin{cases}
𝜆, 覆盖率增加,\\
0, 其他，
\end{cases}
$$
- 多样性奖励用于鼓励命令生成器序列的多样性，通过将命令生成器序列 ct 与 h 最近的序列进行比较来衡量其多样性 divt：
$$
div_{t} = \frac{1}{|𝐶ℎ |} \sum_{c𝑖 ∈ 𝐶ℎ} 𝐷𝑖𝑠𝑡 (𝑐𝑖, 𝑐𝑡 )
$$
其中𝐶ℎ = {𝑐𝑡 −ℎ, . . . , 𝑐𝑡 −1 }表示与当前序列𝑐𝑡最接近的一组指令生成序列，本研究中设定ℎ = 100。距离函数𝐷𝑖𝑠𝑡 (𝑐𝑖, 𝑐𝑡 ) = 1 − 𝑐𝑜𝑠𝑖𝑛𝑒 (𝑉𝑖, 𝑉𝑡 )，其中𝑉𝑖和𝑉𝑡分别为𝑐𝑖和𝑐𝑡的向量表示，𝑐𝑜𝑠𝑖𝑛𝑒 (𝑉𝑖, 𝑉𝑡 )表示二者的余弦相似度。当前指令生成序列𝑐𝑡的多样性奖励定义为：
$$
R_{div}^t = \frac{1}{h} \sum_{i=1}^h (𝑑𝑖𝑣_{𝑡} − 𝑑𝑖𝑣_{𝑡−𝑖} )
$$

最终的奖励是：
$$
Reward (t) = 𝑅_{crash}^t + 𝑅_{cov}^t + 𝑅_{div}^t. 
$$

获得实际奖励后，GzFuzz 进一步使用评判者模块来获取预测的潜在奖励。受现有研究 [15, 16] 的启发，我们考虑使用优势损失函数来减少神经网络的高方差。更具体地说，对于评判者模块，损失使用均方误差计算：
Advantage(𝑡) = Reward (𝑡) − y𝑐𝑟𝑖𝑡𝑖𝑐, 
alueLoss(𝑡) = Advantage(𝑡)^2

其中 Reward(t)Reward(t) 是实际奖励，ycriticycritic​ 是评判者模块在迭代 tt 时预测的奖励。对于行动者和子行动者，损失使用策略梯度方法更新：
PolicyLoss(t)=−log(Pθ​(s,ct​))⋅Advantage(t),

其中 Pθ(s,ct)Pθ​(s,ct​) 是在输入特征 ss 下选择 ctct​ 的转移概率，它是由网络参数 θθ 下对应的行动者/子行动者预测的。

行动者/子行动者和评判者模块都使用反向传播来更新参数，目标是最大化累积奖励。请注意，对于子行动者，只更新那些对应于所选命令生成器的子行动者：
θ=UpdateParameter(θ,PolicyLoss(t),α),
ϕ=UpdateParameter(ϕ,ValueLoss(t),α),

其中 θ 和 ϕ 分别表示行动者/子行动者和评判者模块的参数。在本研究中，我们利用 Adam 优化器来更新参数，α 表示神经网络模块的初始学习率。

对于上述参数，在下图中介绍了 GzFuzz 中引入的参数设置。ω的值是根据现有研究设置的，GzFuzz 的其余参数设置基于一个小规模实验。在我们的初步实验中，我们观察到 GzFuzz 对参数设置不太敏感。

![](../images/learning/2025_issta_gzfuzz/param.png)

### 三、实例

下图展示了一个已修复的真实错误。具体来说，图 4(a) 显示了一个带有 AckermannSteering 插件的模型片段，而图 4(b) 演示了一个创建并插入此模型的 Gazebo 命令。图 4(c) 展示了一个通过调用服务 /world/world_0/control/ 并附带消息 "reset {all: true}" 来重置模拟的命令。执行后，模拟重新启动，模型 vehicle_blue预计将从场景中移除。然而，由于底层实现的问题，AckermannSteering 插件未能正确卸载，这进一步导致模拟器崩溃。

![](../images/learning/2025_issta_gzfuzz/example.png)

为了触发此错误，基于学习的命令生成器选择首先通过行动者模块选择生成器 6 和 1。生成器 6 需要其对应的子行动者指定 vehicle_blue 模型作为参数，而生成器 1 选择 world/world_@/control 服务，并基于 Protobuf 定义构建一个重置消息。语法感知的可行命令生成机制确保两个命令都符合 Gazebo 的语法要求。执行这些命令会触发该错误，该错误源于 Gazebo 对级联插件移除的错误处理。

### 四、实验环境

GzFuzz 使用 Python 实现。神经网络模块使用 PyTorch实现。对于 XML 解析和修改，使用了 lxml 库 (5.3.0)。为了为给定的 Protobuf 定义生成可行消息，使用了 randomproto 库 (0.0.1)。至于硬件环境，我们的实验在一台运行 Ubuntu 22.04 (x86_64) 的 GNU/Linux PC 上进行，该 PC 配备 Intel Core i9-13900K CPU 和 128GB RAM。
