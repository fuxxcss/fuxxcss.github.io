---
title: "RobotAgent：大脑（Part 2/6）"
excerpt: '机器人智能体的大脑'

collection: theory
category: robotagent
permalink: /theory/robotagent/brain
tags: 
  - robotagent

layout: single
read_time: true
author_profile: false
comments: true
share: true
related: true
---

![](../../images/theory/robotagent/brain/transformer.png)

## RobotAgent的组件

在行动阶段，RobotAgent发展出了一套完整的具身系统，其中包括4个核心组件[^1]：
1. 认知核心，是RobotAgent的大脑，集成学习、推理、规划、决策和反思的能力。
2. 记忆系统，为认知核心服务，包括短期记忆和长期记忆，支持终身学习。
3. 感知系统，从环境中获取视觉、听觉、触觉等多模态传感器数据，作为模型的输入。
4. 行动系统，实现与物理环境的交互，这包括执行代码、控制机器人肢体、在物理世界中导航等。将认知核心的决策转化为具体的、可在环境中执行的操作序列。

一个智能体在离散时间步 t 中运行，不断地与其环境相互作用。在每一步中，发生以下过程：

![](../../images/theory/robotagent/brain/io.png)

1. 环境状态 (st ∈ S) 
2. 感知系统（P），agent感知环境生成观察结果 $ot， ot = P(st, Mt−1) $,其中 Mt−1 引导选择性注意和过滤。
3. 认知核心（C），包含两个流程：
- 学习（L），基于前一个行为和观察，智能体更新心智状态 $ Mt，Mt = L(Mt−1, at−1, ot)$，其中心智状态包括：记忆系统，生成式模型，规划的目标和反思需要的奖励，Mt = {Mt 记忆, Mt 模型参数，Mt 规划的目标，Mt 奖励，... }。
- 推理（R），决定接下来的行动，at或者A = R(Mt)，分为内外两种类型的行动：
  - 外部行动，将直接影响环境。
  - 内部行动，包括，1）规划，未来行动的序列；2）决策，从可用的选项中选择最佳的行动。
4. 行动系统，将行动 at 转换为可执行的形式，a′t = E(at)。
5. 环境转变（T），环境对agent的行动做出反应，st+1 = T(st, a′t)。
6. （多智能体场景中）每个智能体 i 维护各自的状态（M it、a it、o it），并且环境根据所有智能体的动作进行集体更新。

RobotAgent实现自主行动的关键在于，通过上述四个模块的协作，RobotAgent能够通过与环境的实时交互和自我反思（如通过强化学习更新策略），持续优化其模型和策略，最终实现鲁棒且准确的任务执行。

## 认知核心

### 一、学习模块

在学习阶段，RobotAgent发展出了生成式模型，它作为一个认知核心，能够从多模态的传感器输入中学习物理世界的规律，通过自监督学习（反向传播和梯度下降）等方式调整其内部参数，为机器人的推理、规划和决策提供了知识基础。要想构建一个高可用性的生成式模型，要有一个学习范式，还要有工程上实现的架构，在架构上运行学习范式，构建出一个落地的生成式模型。

#### 1.生成式模型

成熟的生成式模型如下：
- LLM：大语言模型，输入文本，输出文本，擅长语义推理，但缺乏物理世界的具身体验。
- VLM：视觉语言模型，输入图像/视频，输出文本描述或回答，提供场景理解能力。
- VLA：视觉-语言-行动模型，输入视觉和语言指令，输出机器人控制指令，在RobotAgent中效果显著，是当前主流方向。

尚有发展空间的生成式模型：
- RFM，机器人基础模型，输入传感器信号，输出预测的机器人控制指令，是RobotAgent的热点研究。
- LAM，大行动模型，输入交互环境，输出预测的可执行行动，是RobotAgent的热点研究。
- WM，世界模型，输入环境状态（视觉，听觉，触觉），输出预测的未来状态，是RobotAgent的热点研究。

#### 2.学习范式

全学习范式，修改整个心智状态 Mt：
- SSL，自监督学习，通过设计 pretext task（如图像掩码重建、时序排序）从无标签数据中学习通用的表征，这些表征可作为下游任务（如监督学习、强化学习）的高质量起点。
- RL，强化学习，智能体通过与环境交互，根据获得的奖励信号来学习最优行动策略。其目标是最大化累积奖励。
  - PPO算法：一种常用的策略梯度算法，在稳定性和性能间取得良好平衡。
  - Q-Learning：一种经典的基于值函数的离线强化学习方法，用于学习行动的价值。
- SFT，监督微调，在预训练后对模型进行监督学习，以使其适应特定任务。

部分学习范式，修改部分心智状态 Mt：
- TL，工具学习，赋予模型使用外部工具的能力，包括工具发现、工具创造、工具使用。

#### 3.架构

架构是学习范式在工程上的具体实现：
- transformer架构，通过自注意力（Self-Attention）机制将学习问题转化为序列建模问题，能有效捕捉长距离依赖关系。其核心组件包括：
  - 多头自注意力：并行地在多个子空间计算注意力，使模型能够同时关注来自不同位置的不同方面的信息。
  - 前馈神经网络：对自注意力层的输出进行非线性变换和特征加工，是每个token独立进行的处理层。
  - 向量嵌入层：作为模型的输入接口，将离散的符号（如词、图像块）转换为连续的、富含语义的向量表示。
- RT系列架构，专为机器人控制设计的Transformer架构变体。它通常将相机图像、自然语言指令和历史动作序列 token化，并通过Transformer进行序列建模，直接输出机器人动作。
- V-JEPA架构，基于JEPA原理，专门为从视频中学习世界模型而设计。

以RT-2架构为例，预训练时，架构的自注意机制保留了每个token级别的上下文信息，自监督学习将最后一个token作为掩码，预测该token形成标签。训练好生成式模型后，输入实时图像用作生成，预测对应的行动，然后附加该行动后的图像，再进行预测。

### 二、推理模块

推理是智能体利用已学知识，通过逻辑推导来增强决策效果、解决复杂问题的过程。它能够充分激发模型上下文中的信息，提高行动预测的准确性和可解释性。
- ReAct框架：将推理和行动在循环中结合。智能体首先生成推理轨迹（对当前状况的思考），再根据思考执行行动，并根据环境反馈进行下一轮推理，以此提高任务解决的准确性。
- CoT框架：通过要求模型生成一系列中间推理步骤，最终得到答案，提升了复杂问题的解决能力。其演进如CoVE 通过使用外部验证器来精炼推理链，进一步提高了可靠性和可解释性。
- ToT框架：将推理过程建模为树形结构，允许模型在思考时探索多种推理路径，并进行前瞻性评估，适用于需要多步规划的问题。

从形式上看，推理过程 R 可以表示为：
1. 行动决策：决定接下来的行动 a_t 或行动序列 A，即 A = R(M_t)。
2. 结构化推理：*R_s = R_1 ◦ R_2 ◦ ... ◦ R_n*，其中每个 R_i 表示具有明确逻辑依赖关系的离散推理步骤，过程透明且可追溯。
3. 非结构化推理：R_u = f (M_t)，推理过程保持隐式和灵活，作为一个黑箱函数动态适应上下文。

### 三、规划模块

规划模块负责制定从初始状态 S_0 到目标状态 S_g 的行动序列。它承接推理模块的输出（如目标分解），生成一个可行的行动路径：S_0 → {a_1, a_2, ..., a_n} → S_g，其中 S0 是开始状态，A = {a1, a2, . . . , an}表示行动空间，Sg 是规划的目标状态。

### 四、决策模块

决策模块在规划模块产生的行动序列基础上，处理具体情境下的即时选择。它具备高度的实时性，负责在规划好的序列框架内，根据当前最新的环境状态，选择并触发最优的单一行动 a_t。

决策函数 D 从规划的当前可选行动集合 *A = {a_1, a_2, ..., a_n}* 中选择最佳行动：a_t = D(a_i)。这个过程可能涉及快速的效用评估或策略查询。

### 五、反思模块

反思模块使智能体能够评估自身行动的表现，并从成功与失败中学习，从而实现持续的自我改进。

强化学习是实现反思的一种核心机制，通过更新价值函数和策略来优化未来行为。其核心是奖励函数的设计：
- 外部奖励：由环境直接提供，如任务成功与否。
- 内部奖励：由智能体内部产生，用于鼓励探索、好奇心或减少不确定性。
- 混合奖励：结合内外部奖励，平衡任务达成与学习效率。

RLVR 等方法引入了“验证器”，以编程方式或模型评估行动输出的质量，为策略优化提供更丰富的训练信号。

从形式上看，反思过程可以建模为一个马尔可夫决策过程 M = (S, A, P, R, γ)，其中 S 表示状态空间， A  表示行动空间，r(s, a) 指定奖励函数，γ ∈ [0, 1] 是折现因子。

## 记忆系统

记忆系统为认知核心服务，其核心功能如下：
- 维护上下文
- 从经验中学习
- 随着时间的推移，保持行动的一致性
- 结构化检索
- 知识蒸馏
- 选择性遗忘

记忆系统的类型：
- 瞬时记忆，
- 记忆缓冲区，缓存短期的工作记忆。
- 矢量数据库，存储长期记忆。

![](../../images/theory/robotagent/brain/memory.png)

记忆生命周期：
1. 编码与保留：通过获取、编码和知识推导，将信息存入记忆系统。
2. 检索与利用：通过记忆匹配、神经记忆网络等机制，在需要时唤回相关信息。

[^1]: Embodied AI Agents https://arxiv.org/abs/2506.22355