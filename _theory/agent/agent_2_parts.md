---
title: "Agent：实现（Part 2/4）"
excerpt: '智能体的组件'

collection: theory
category: 
permalink: /theory/agent/parts
tags: 
  - agent

layout: single
read_time: true
author_profile: false
comments: true
share: true
related: true
---

![](../../images/theory/agent/transformer.png)

## agent的组件

在行动阶段，agent发展出了一套完整的系统，其中包括4个组件：
1. 认知核心，是agent的大脑，集成学习、推理、规划、决策和反思的能力。
2. 记忆系统，记忆系统为认知核心服务，有缓冲区和数据库两种存储类型。
3. 感知系统，从环境中获取文本，视觉、听觉、触觉等多模态数据，作为训练数据或者作为生成时的输入。
4. 行动系统，实现与环境的交互，这包括生成自然语言、执行代码、控制机器人肢体、在虚拟世界中导航等。将认知核心的决策转化为具体的、可在环境中执行的操作序列。

```
数学模型： 
一个智能体在离散时间步 t 中运行，不断地与其环境相互作用。在每一步中，发生以下过程：
1. 环境状态 (st ∈ S)
2. 感知系统（P），agent感知环境生成观察结果 ot，ot = P(st, Mt−1),其中 Mt−1 引导选择性注意和过滤。
3. 认知核心（C），包含两个流程：
- 学习（L），基于前一个行为和观察，智能体更新心智状态，Mt = L(Mt−1, at−1, ot)，其中心智状态包括：记忆系统，生成式模型，规划的目标和反思需要的奖励，Mt = {Mt 记忆, Mt 模型参数，Mt 规划的目标，Mt 奖励，... }。
- 推理（R），决定接下来的行动，at或者A = R(Mt)，分为内外两种类型的行动：
  - 外部行动，将直接影响环境。
  - 内部行动，包括，1）规划，未来行动的序列；2）决策，从可用的选项中选择最佳的行动。
4. 行动系统，将行动 at 转换为可执行的形式，a′t = E(at)。
5. 环境转变（T），环境对agent的行动做出反应，st+1 = T(st, a′t)。
6. （多智能体场景中）每个智能体 i 维护各自的状态（M it、a it、o it），并且环境根据所有智能体的动作进行集体更新。
```
![](../../images/theory/agent/io.png)

完整系统的agent关键在于，通过上述四个模块的协作。agent能够通过与环境的交互和自我反思（强化学习），持续学习（模型的训练）确保决策的不断改进，最终实现准确的行动。

### 一、认知核心

**1.学习模块**

在学习（Learning）阶段，agent发展出了生成式模型，还没有一个认知核心，生成式模型具有学习的能力，从多模态的输入中学习，通过反向传播和梯度下降调整模型的参数，为agent的推理、规划和决策提供了知识基础。成熟的生成式模型：
- LLM，大语言模型，输入文本，输出预测的文本。
- SLM，小语言模型，输入文本，输出预测的文本。
- VLM，视觉语言模型，输入图像视频，输出预测的文本，或者反过来。

尚有发展空间的模型：
- LMM，多模态大模型，输入多模态，输出预测的多模态。
- RFM，机器人基础模型，输入传感器信号，输出预测的机器人控制指令。
- LAM，大行动模型，输入交互环境，输出预测的可执行行动。
- WM，世界模型，输入环境状态，输出预测的未来状态。

```
数学模型：
1. 基于前一个行为和观察，智能体更新心智状态 Mt = L(Mt−1, at−1, ot)。
其中，M = {M 记忆, M 模型参数，M 规划的目标，M 奖励，... }
```

要想构建一个高可用性的生成式模型，要有一个学习范式，还要有工程上实现的架构，在架构上运行学习范式，构建出一个落地的生成式模型。

全学习范式，修改整个 M ：
- SSL，自监督学习，首先无监督学习，设计假托任务生成隐式标签，然后迁移到监督学习、强化学习（等下游任务）上训练生成式模型。
- RL，强化学习，在环境中采取行动并根据获得的奖励来学习最优的策略。
- SFT，监督微调，在预训练后对模型进行监督学习。

部分学习范式，修改部分 M：
- ICL，上下文学习，在不修改 M 模型参数 的情况下，有效地增强认知能力。
- TL，工具学习，包括工具发现、工具创造、工具使用。

架构：
- transformer架构，将学习范式转换为序列建模问题，提供自注意机制学习输入间的关系。
- GPT、BERT架构，transformer的改良版。

以LLM为例，预训练时，transformer架构的自注意机制保留了每个token级别的上下文信息，自监督学习将最后一个token作为掩码，预测该token形成标签。训练好生成式模型后，输入一段文本用作生成，预测输入的下一个token，然后附加该token再进行预测。

**2.推理模块**

推理（Reasoning）是增强模型生成效果的手段，它充分激发了模型学习的上下文信息，预测出的token准确性提高。

结构化推理：
- ReAct框架，循环进行 推理-行动-反馈，提高了准确性。
- CoT框架，演化出CoVE，提高了可解释性。
- ToT框架，树型结构。

```
数学模型：
1. 决定接下来的行动，at或者A = R（Mt）
2. 对于结构化推理，Rs = R1 ◦ R2 ◦ ... ◦ Rn，其中每个Ri表示具有明确逻辑依赖关系的离散推理步骤。
3. 对于非结构化推理，Ru = f (Mt)，保持隐式和灵活，允许动态适应上下文。
```

**3.规划模块**

规划模块从推理中来。llm通常缺乏对世界动态的深入理解，依赖于模式识别而不是真正的因果推理，这妨碍了它们管理子目标交互和环境变化的能力。

```
数学模型：
1. 构建从初始状态到期望目标状态的潜在路径的过程 S0 → {a1, a2, . . . , an} → Sg 
其中 S0 是开始状态，A = {a1, a2, . . . , an}表示行动空间，Sg 是规划的目标状态。
```

**4.决策模块**

决策模块通常处理的是规划通过任务分解，得到的当前执行的动作序列，具备即时性。

```
数学模型：
1. 从可用的选项中选择最佳的行动，at = D（ai）
其中 ai 来自规划的当前行动序列 A = {a1, a2, . . . , an}
```

**5.反思模块**

强化学习是实现反思的一个方案，更新奖励状态：
- 外部奖励
- 内部奖励
- 混合奖励
- 分层模型

```
数学模型：
1. M 奖励 = (S, A, r, γ),
其中 S 表示状态空间， A  表示行动空间，r(s, a) 指定奖励函数，γ ∈ [0, 1] 是折现因子。
```

### 二、记忆系统

记忆系统为认知核心服务，其核心功能如下：
- 维护上下文
- 从经验中学习
- 随着时间的推移，保持行动的一致性
- 结构化检索
- 知识蒸馏
- 选择性遗忘

```
数学模型：
1. M 记忆
```

记忆系统的类型：
- 瞬时记忆，
- 记忆缓冲区，缓存短期的工作记忆。
- 矢量数据库，存储长期记忆。

![](../../images/theory/agent/memory.png)

记忆的生命周期：
1. 保留
- 获取、编码和推导
2. 检索
- 记忆匹配、神经记忆网络和记忆利用

### 三、感知系统

agent从环境中获取文本，视觉、听觉、触觉等多模态数据，作为训练数据或者作为生成时的输入。

模态数据：
- 文本
- 图像，视频，音频
- 其他，如受到嗅觉、触觉启发

```
数学模型：
1. agent感知环境生成观察结果 ot，ot = P(st, Mt−1),其中 Mt−1 引导选择性注意和过滤。
```

### 四、行动系统

系统的核心是建立与环境的交互，这包括生成自然语言、执行代码、控制机器人肢体、在虚拟世界中导航等。将认知核心的决策转化为具体的、可在环境中执行的操作序列，然后通过预定义的奖励函数从收集的动作轨迹中实现学习过程。

![](../../images/theory/agent/action.png)

```
数学模型：
1. 行动系统将行动 at 转换为可执行的形式，a′t = E(at)。
```

行动系统包括三个主要部分：
1. 行动空间，包括智能体在现实世界场景或下游任务中可以执行的所有类型的行动。
2. 动态环境下的学习。
3. 工具空间，包含了agent可以执行用于利用的工具、接口或中间件，范围从机械臂等物理设备到api等数字接口。

行动空间包括以下3个空间，分别对应各自的工具空间：
- 语言空间，通过语言驱动的行动进行操作，如推理、编程、检索信息、执行API调用或与外部工具交互。
- 数字空间，在数字环境中操作，将行动空间扩展到包括web浏览、GUI交互、移动应用程序和具身系统。
- 物理空间，与真实物理世界交互，智能体能够处理来自真实世界环境的信号，并生成反馈。

## agent实例

一个具体的google agent系统如下图所示，编排层、模型层实现了认知核心和记忆系统，感知系统和行动系统则通过工具层实现。

![](../../images/theory/agent/google_arch.png)

### 一、模型层

agent中，模型层指的是 LM（语言模型），作为认知核心，实现学习、推理、规划、决策和反思，模型层可以是一个或多个 LM，大小不限。

### 二、编排层

编排一个循环过程，控制着agent如何接收信息、执行一些内部推理，并利用这些推理为下一步行动或决策提供信息，这个循环一直持续到达到目标或停止点为止。

例如，agent使用 ReAct 框架为用户查询选择合适的行动和工具，那么事件序列如下面发生的：

![](../../images/theory/agent/google_reasoning.png)

1. 用户向智能体发送查询。
2. 智能体开始 ReAct 序列。
3. 智能体向模型提供提示，要求其生成下一个 ReAct 步骤及其相应输出：
- a. 问题： 来自用户查询的输入问题，随提示词一同提供。
- b. 思考： 模型关于下一步应做什么的思考。
- c. 行动： 模型对下一步采取何种行动的决策。
  - i. 此处可以进行工具选择。
  - ii. 例如，一个行动可以是 [航班, 搜索, 代码, 无] 中的一个，其中前三个代表模型可以选择的一个已知工具，最后一个代表"无工具选择"。
- d. 行动输入： 模型决定向工具提供什么输入（如果有的话）。
- e. 观察： 行动 / 行动输入序列的结果。
  - i. 这个 思考 / 行动 / 行动输入 / 观察 序列可以根据需要重复 N 次。
- f. 最终答案： 模型为原始用户查询提供的最终答案。
4. ReAct 循环结束，并向用户返回最终答案。

虽然模型可以根据自己的先验知识猜测答案（幻觉），但它却使用了一种工具（航班）来搜索实时的外部信息。这种外部信息，使其能够根据实际数据做出更明智的决定，并将这些信息汇总反馈给用户。

### 三、工具层

AIGC无法与环境交互，工具弥补了这一缺陷，使agent能够与外部数据和服务进行交互，开启更广泛的可能性。如检索增强生成（RAG），用来扩展agent的能力。

google agent可以与三种主要工具类型进行交互：扩展、函数和数据存储。通过为agent配备工具，使其不仅能理解环境，还能根据环境采取行动，从行动中反思。

**1.扩展**

扩展通过以下方式在agent和应用程序接口之间架起了桥梁：
- 使用示例教agent如何使用 API 端点。
- 告诉agent成功调用 API 端点需要哪些参数。

![](../../images/theory/agent/google_extension.png)

扩展可以独立于agent而设计，但应作为agent配置的一部分提供。agent在运行时使用模型和示例来决定哪个扩展（如果有的话）适合解决用户的查询。

![](../../images/theory/agent/google_one2many_extension.png)

**2.函数**

模型可以接受一组已知函数，并根据其规范决定何时使用每个函数以及函数需要哪些参数。函数与扩展有一些不同之处，其中最明显的是:
- 模型会输出一个函数及其参数，但不会调用实时 API，函数可能调用多个内部 API。
- 函数在客户端执行，而扩展在agent端执行。

这里的函数通常以json等格式化形式存在，被agent生成发送到客户端，客户端负责调用函数。

![](../../images/theory/agent/google_func_call.png)

**3.数据存储**

数据存储通常以矢量数据库的形式实现，数据存储与语言模型结合使用的最多的例子之一是检索增强语言模型（RAG）的实施，旨在通过让模型访问各种格式的数据，从而扩展模型知识的广度和深度，使其超越基础训练数据：
- 网站内容。
- 结构化数据格式，如 PDF、Word 文档、CSV、电子表格等。
- HTML、PDF、TXT 等格式的非结构化数据。

![](../../images/theory/agent/google_datastore.png)

每个用户请求和agent响应循环的基本流程：
1. 用户查询被发送给嵌入模型，以生成该查询的向量嵌入。
2. 随后，使用如 SCaNN 之类的匹配算法，将查询向量与向量数据库中的内容进行匹配。
3. 匹配出的内容以文本格式从向量数据库中检索出来，并发送回智能体。
4. 智能体同时接收到用户查询和检索到的内容，随后制定回应或决定下一步行动。
5. 最终响应被发送给用户。

如下图，展示了ReAct实施RAG的例子。

![](../../images/theory/agent/google_rag.png)

### 四、快速构建

为了提供真实世界中可执行的agent操作示例，可以使用 LangChain 和 LangGraph 库构建一个快速原型。这些流行的开源库允许用户通过将逻辑、推理和工具调用的序列 “链”在一起来构建客户agent，从而回答用户的询问。

---
