---
title: "RobotAgent：进化论（Part 1/5）"
excerpt: '过去，现在和未来'

collection: theory
category: robotagent
permalink: /theory/robotagent/view
tags: 
  - robotagent

layout: single
read_time: true
author_profile: false
comments: true
share: true
related: true
---

![](../../images/theory/robotagent/view/brain.png)

## 前言

### 一、Agent层次

人工智能的发展受到人脑研究的启发[^1]，理解人脑功能和人工智能之间的相似之处，揭示了人工智能的优势和当前的局限性，特别是大型语言模型（llm），截至2025年，人工智能的研究状态可以分为三个不同的层次：
- 一级（L1）：目前人工智能发展良好。
- 二级（L2）：适度探索，有部分进展。还可以进一步改进。
- 三级（L3）：很少探索；有很大的研究空间。

从人脑的结构及其功能出发：
- 额叶，执行控制与认知：决策规划（L2）、逻辑推理（L2）、工作记忆（L2）、自我意识（L3）、认知灵活性（L3）和抑制控制（L3）。
- 顶叶，空间处理与多感觉整合：注意力（L2）、空间定向（L2）、感觉运动协调（L2）和细节触觉感知（L3）。
- 枕叶，视觉处理：视觉感知（L1）、场景理解（L2）。
- 颞叶，语言、记忆和听觉处理：听觉处理（L1）、语言理解（L1）、记忆形成（L2）和语义理解（L2）。
- 小脑，协调和运动学习：运动协调（L2）、精确技能学习（L2）、自适应错误纠正（L2）和认知定时（L3）。
- 脑干，自主调节和反射控制：维持生命的基本自主功能（L3）和快速反射反应（L1）。
- 边缘系统，情感、同理心和动机：情绪处理（L3）、奖励机制（L2）、共情（L3）、压力调节（L3）和动机驱动（L3）。

即当前人工智能的发展合并了有研究进展的模块，生物机制作为灵感而不是直接复制：
- 认知核心：语言理解（L1）、语义理解（L2）、注意力（L2）、奖励机制（L2）、决策规划（L2）、逻辑推理（L2）。
- 记忆系统：工作记忆（L2）、记忆形成（L2）。
- 感知系统：视觉感知（L1）、听觉处理（L1）、场景理解（L2）、空间定向（L2）。
- 行动系统：快速反射反应（L1）、运动协调（L2）、精确技能学习（L2）、自适应错误纠正（L2）。

### 二、Robot层次

现代人工智能，把机器人发展路上的几乎所有障碍，都转化成了“数据问题”。这意味着，机器人不再是死板的执行器，而是能从真实世界中不断学习、进化、适应。机器人发展的五级自治[^2]：
- 一级（L1），脚本化运动：固定编程，只能在封闭的“单元格”中工作，完全没有自主性。
- 二级（L2），智能抓取与放置：核心突破是“感知”，机器人不再依赖固定的环境，而是能识别杂乱堆放的物品，判断其位置和姿态，并调整抓取动作。
- 三级（L3），自主移动：关键技术是视觉语言模型（VLM），机器人可以在开放、混乱、动态的真实世界中自主规划路径、理解场景。
- 四级（L4），低技能操作：关键技术是视觉-语言-动作模型（VLA），VLA[^3]在VLM的基础上，增加了“动作”模态，可以直接输出操作指令，机器人不仅能走到任务地点，还能与环境进行有意义的互动：开门、拿杯子、搅拌锅里的食物、折叠毛巾。
- 五级（L5），力觉依赖型任务：2025年尚处于研究阶段，机器人发展的终极目标，高技能任务，比如进行复杂的管道安装或电路接线，这些任务仅靠视觉无法实现，还需要触觉和力反馈。

自三级（L3）开始，实现了Robot到RobotAgent的转变。

## RobotAgent进化论

RobotAgent从过去专注于环境理解的学习阶段，发展到现在的自主行动阶段，未来将达到自我优化的进化阶段[^4]：
1. 可以学习（过去）
2. 可以行动（现在）
3. 可以进化（未来）

### 一、学习阶段

**关键技术（生成式模型，学习范式，推理）**

在学习阶段，RobotAgent发展出了生成式模型，它作为一个认知核心，能够从多模态的传感器输入中学习物理世界的规律，通过自监督学习（反向传播和梯度下降）等方式调整其内部参数，为机器人的推理、规划和决策提供了知识基础。

成熟的生成式模型如下：
- LLM：大语言模型，输入文本，输出文本，擅长语义推理，但缺乏物理世界的具身体验。
- VLM：视觉语言模型，输入图像/视频，输出文本描述或回答，提供场景理解能力。
- VLA：视觉-语言-行动模型，输入视觉和语言指令，输出机器人控制指令，在RobotAgent中效果显著，是当前主流方向。

尚有发展空间的生成式模型：
- RFM，机器人基础模型，输入传感器信号，输出预测的机器人控制指令，是RobotAgent的热点研究。
- LAM，大行动模型，输入交互环境，输出预测的可执行行动，是RobotAgent的热点研究。
- WM，世界模型，输入环境状态（视觉，听觉，触觉），输出预测的未来状态，是RobotAgent的热点研究。

要想构建一个高可用性的生成式模型，要有一个学习范式，还要有工程上实现的架构，在架构上运行学习范式，构建出一个落地的生成式模型。

全学习范式：
- SSL，自监督学习，首先无监督学习，设计假托任务生成隐式标签，然后迁移到监督学习、强化学习（等下游任务）上训练生成式模型。
- RL，强化学习，在环境中采取行动并根据获得的奖励来学习最优策略。
- SFT，监督微调，在预训练后对模型进行监督学习。

部分学习范式：
- TL，工具学习，包括工具发现、工具创造、工具使用。

架构：
- transformer架构，将学习范式转换为序列建模问题，提供自注意机制学习输入间的关系。
- RT系列架构，用于机器人控制的transformer的定制版。
- V-JEPA架构，基于JEPA原理，专门为从视频中学习世界模型而设计。

以RT-2架构为例，预训练时，架构的自注意机制保留了每个token级别的上下文信息，自监督学习将最后一个token作为掩码，预测该token形成标签。训练好生成式模型后，输入实时图像用作生成，预测对应的行动，然后附加该行动后的图像，再进行预测。

推理（Reasoning）是增强模型决策效果的关键手段，它充分激发了模型学习的上下文信息，预测出的结果准确性提高。
- ReAct框架，循环进行 推理-行动-反馈，提高了准确性。
- CoT框架，演化出CoVE，提高了可解释性。
- ToT框架，树型结构。

### 二、行动阶段

**关键技术（规划，决策，反思，记忆，感知，行动）**

在行动阶段，RobotAgent发展出了一套完整的具身系统，其中包括4个核心组件：
1. 认知核心，是RobotAgent的大脑，集成学习、推理、规划、决策和反思的能力。
2. 记忆系统，为认知核心服务，包括短期记忆和长期记忆，支持终身学习。
3. 感知系统，从环境中获取视觉、听觉、触觉等多模态传感器数据，作为模型的输入。
4. 行动系统，实现与物理环境的交互，这包括执行代码、控制机器人肢体、在物理世界中导航等。将认知核心的决策转化为具体的、可在环境中执行的操作序列。

RobotAgent实现自主行动的关键在于，通过上述四个模块的协作，RobotAgent能够通过与环境的实时交互和自我反思（如通过强化学习更新策略），持续优化其模型和策略，最终实现鲁棒且准确的任务执行。

### 三、进化阶段

**关键技术（优化空间，终身学习，仿真到真实迁移）**

在进化阶段，RobotAgent实现了系统的自优化与终身学习，其优化空间包含两个层面：
1. 模块级优化空间，其中有3个关键模块可以被持续优化：
- 模型优化：通过持续的交互数据， refine 其对物理规律的预测能力，这是最核心的优化目标。
- 技能与策略优化：通过RL等方法，在仿真或真实环境中学习新的技能或优化现有策略。
- 感知-行动回路优化：优化从感知到控制指令的整个管道的效率和准确性。
2. 系统级优化空间，专注于优化整个Agent系统的综合性能：
- 仿真到真实迁移：通过在多样化仿真环境中训练，并优化其向真实世界的迁移能力。
- 多任务与泛化能力：优化系统在新任务、新环境中的零样本或小样本适应能力。


[^1]: Foundation Agents https://arxiv.org/abs/2504.01990
[^2]: Robotics Levels of Autonomy https://semianalysis.com/2025/07/30/robotics-levels-of-autonomy/
[^3]: VLA model for Robot https://arxiv.org/abs/2307.15818
[^4]: Embodied AI Agents https://arxiv.org/abs/2506.22355